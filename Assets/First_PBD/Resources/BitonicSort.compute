
#define THREADS 128

#define BITONIC_BLOCK_SIZE 512
#define TRANSPOSE_BLOCK_SIZE 16

int Level;
int LevelMask;
int Width;
int Height;


RWStructuredBuffer<int2> Data;
StructuredBuffer<int2> Input;

groupshared int2 shared_data[BITONIC_BLOCK_SIZE];
groupshared int2 transpose_shared_data[TRANSPOSE_BLOCK_SIZE * TRANSPOSE_BLOCK_SIZE];
//////test tiling
int dispatchgroupdimX;

#define DXC_STATIC_DISPATCH_GRID_DIM 0


uint2 ThreadGroupTilingX(const uint2 dipatchGridDim, // Arguments of the Dispatch call (typically from a ConstantBuffer)
	const uint2 ctaDim,			// Already known in HLSL, eg:[numthreads(8, 8, 1)] -> uint2(8, 8)
	const uint2 groupThreadID,		// SV_GroupThreadID
	const uint2 groupId             // SV_GroupID
)
{
	const uint maxTileWidth = 8;		// User parameter (N). Recommended values: 8, 16 or 32.
	// A perfect tile is one with dimensions = [maxTileWidth, dipatchGridDim.y]
	const uint Number_of_CTAs_in_a_perfect_tile = maxTileWidth * dipatchGridDim.y;

	// Possible number of perfect tiles
	const uint Number_of_perfect_tiles = dipatchGridDim.x / maxTileWidth;

	// Total number of CTAs present in the perfect tiles
	const uint Total_CTAs_in_all_perfect_tiles = Number_of_perfect_tiles * maxTileWidth * dipatchGridDim.y;
	const uint vThreadGroupIDFlattened = dipatchGridDim.x * groupId.y + groupId.x;

	// Tile_ID_of_current_CTA : current CTA to TILE-ID mapping.
	const uint Tile_ID_of_current_CTA = vThreadGroupIDFlattened / Number_of_CTAs_in_a_perfect_tile;
	const uint Local_CTA_ID_within_current_tile = vThreadGroupIDFlattened % Number_of_CTAs_in_a_perfect_tile;
	uint Local_CTA_ID_y_within_current_tile;
	uint Local_CTA_ID_x_within_current_tile;

	if (Total_CTAs_in_all_perfect_tiles <= vThreadGroupIDFlattened)
	{
		// Path taken only if the last tile has imperfect dimensions and CTAs from the last tile are launched. 
		uint X_dimension_of_last_tile = dipatchGridDim.x % maxTileWidth;
#ifdef DXC_STATIC_DISPATCH_GRID_DIM
		X_dimension_of_last_tile = max(1, X_dimension_of_last_tile);
#endif
		Local_CTA_ID_y_within_current_tile = Local_CTA_ID_within_current_tile / X_dimension_of_last_tile;
		Local_CTA_ID_x_within_current_tile = Local_CTA_ID_within_current_tile % X_dimension_of_last_tile;
	}
	else
	{
		Local_CTA_ID_y_within_current_tile = Local_CTA_ID_within_current_tile / maxTileWidth;
		Local_CTA_ID_x_within_current_tile = Local_CTA_ID_within_current_tile % maxTileWidth;
	}

	const uint Swizzled_vThreadGroupIDFlattened =
		Tile_ID_of_current_CTA * maxTileWidth +
		Local_CTA_ID_y_within_current_tile * dipatchGridDim.x +
		Local_CTA_ID_x_within_current_tile;

	uint2 SwizzledvThreadGroupID;
	SwizzledvThreadGroupID.y = Swizzled_vThreadGroupIDFlattened / dipatchGridDim.x;
	SwizzledvThreadGroupID.x = Swizzled_vThreadGroupIDFlattened % dipatchGridDim.x;

	uint2 SwizzledvThreadID;
	SwizzledvThreadID.x = ctaDim.x * SwizzledvThreadGroupID.x + groupThreadID.x;
	SwizzledvThreadID.y = ctaDim.y * SwizzledvThreadGroupID.y + groupThreadID.y;

	return SwizzledvThreadID.xy;
}


//////
#pragma kernel BitonicSort
[numthreads(BITONIC_BLOCK_SIZE, 1, 1)]
void BitonicSort(int3 Gid : SV_GroupID, int3 DTid : SV_DispatchThreadID, int3 GTid : SV_GroupThreadID, int GI : SV_GroupIndex)
{
	//test. use SV_DispatchThreadID and SV_GroupIndex
	//const uint2 dispatchGridDim = { dispatchgroupdimX, 1};
	//const uint2 a1 = { BITONIC_BLOCK_SIZE, 1 };
	//const uint2 a2 = { GTid.x, GTid.y };
	//const uint2 a3 = { Gid.x, Gid.y };
	//DTid.x = ThreadGroupTilingX(dispatchGridDim,a1,a2,a3).x;

	//////
	// Load shared data
	shared_data[GI] = Data[DTid.x];
	GroupMemoryBarrierWithGroupSync();

	// Sort the shared data
	for (int j = Level >> 1; j > 0; j >>= 1)
	{
		int2 result = ((shared_data[GI & ~j].x <= shared_data[GI | j].x) == (bool)(LevelMask & DTid.x)) ? shared_data[GI ^ j] : shared_data[GI];
		GroupMemoryBarrierWithGroupSync();
		shared_data[GI] = result;
		GroupMemoryBarrierWithGroupSync();
	}

	// Store shared data
	Data[DTid.x] = shared_data[GI];
}

#pragma kernel MatrixTranspose
[numthreads(TRANSPOSE_BLOCK_SIZE, TRANSPOSE_BLOCK_SIZE, 1)]
void MatrixTranspose(int3 Gid : SV_GroupID, int3 DTid : SV_DispatchThreadID, int3 GTid : SV_GroupThreadID, int GI : SV_GroupIndex)
{
	transpose_shared_data[GI] = Input[DTid.y * Width + DTid.x];
	GroupMemoryBarrierWithGroupSync();
	int2 XY = DTid.yx - GTid.yx + GTid.xy;
	Data[XY.y * Height + XY.x] = transpose_shared_data[GTid.x * TRANSPOSE_BLOCK_SIZE + GTid.y];
}

#pragma kernel Fill
[numthreads(THREADS, 1, 1)]
void Fill(int DTid : SV_DispatchThreadID)
{
	if (DTid < Width)
		Data[DTid] = Input[DTid];
	else
		Data[DTid] = 0x7FFFFFFF;
}

#pragma kernel Copy
[numthreads(THREADS, 1, 1)]
void Copy(int DTid : SV_DispatchThreadID)
{
	if (DTid < Width)
		Data[DTid] = Input[DTid];
}